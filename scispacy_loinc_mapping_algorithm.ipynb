{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfea90b4",
   "metadata": {},
   "source": [
    "### Use Scispacy model via UMLS API to get the CUis and extract their LOINC parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram approach\n",
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "# UMLS API Key â€” Replace this at runtime - PROVIDE THE API KEY\n",
    "UMLS_API_KEY = \"XXX\"  \n",
    "\n",
    "# Function to clean LOINC part URLs\n",
    "def clean_lnc_url(lnc_url):\n",
    "    return lnc_url.replace(\"https://uts-ws.nlm.nih.gov/rest/content/2024AB/source/LNC/\", \"\")\n",
    "\n",
    "# Function to fetch LOINC parts from UMLS CUI\n",
    "def fetch_lnc_from_umls(cui, api_key):\n",
    "    base_url = f\"https://uts-ws.nlm.nih.gov/rest/content/2024AB/CUI/{cui}/atoms?apiKey={api_key}&sabs=LNC\"\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            results = data.get(\"result\", [])\n",
    "            loinc_parts = []\n",
    "\n",
    "            for item in results:\n",
    "                loinc_part = item.get(\"code\")\n",
    "                term_type = item.get(\"termType\")\n",
    "                if term_type in [\"LPDN\", \"LPN\"] and loinc_part:\n",
    "                    cleaned_part = clean_lnc_url(loinc_part)\n",
    "                    if cleaned_part not in loinc_parts:\n",
    "                        loinc_parts.append(cleaned_part)\n",
    "\n",
    "            return \", \".join(loinc_parts) if loinc_parts else \"No Mapping Found\"\n",
    "        return \"No Mapping Found\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching LOINC Part for CUI {cui}: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "# Load models\n",
    "scibert_nlp = spacy.load(\"en_core_sci_scibert\")\n",
    "scibert_nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "\n",
    "scilg_nlp = spacy.load(\"en_core_sci_lg\")\n",
    "scilg_nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n",
    "\n",
    "print(\"Models loaded successfully!\")\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def extract_entities(doc, nlp_model, row_id, original_text):\n",
    "    linker = nlp_model.get_pipe(\"scispacy_linker\")\n",
    "    found = False\n",
    "    results = []\n",
    "    for ent in doc.ents:\n",
    "        for cui, score in ent._.kb_ents:\n",
    "            concept = linker.kb.cui_to_entity[cui]\n",
    "            loinc_part = fetch_lnc_from_umls(concept.concept_id, UMLS_API_KEY)\n",
    "            results.append({\n",
    "                \"Row_ID\": row_id,\n",
    "                \"Text\": original_text,\n",
    "                \"Start\": ent.start_char,\n",
    "                \"End\": ent.end_char,\n",
    "                \"Label\": ent.label_,\n",
    "                \"UMLS_CUI\": concept.concept_id,\n",
    "                \"UMLS_Name\": concept.canonical_name,\n",
    "                \"Score\": score,\n",
    "                \"LOINC_Part\": loinc_part\n",
    "            })\n",
    "            found = True\n",
    "    return results, found\n",
    "\n",
    "def sliding_window_linking(text, nlp_model, row_id, max_n=3):\n",
    "    tokens = [token.text for token in nlp_model.tokenizer(text)]\n",
    "    all_results = []\n",
    "    found_any = False\n",
    "    # Generate ngrams from 1 to max_n and run linking\n",
    "    for n in range(1, min(max_n, len(tokens)) + 1):\n",
    "        ngrams = generate_ngrams(tokens, n)\n",
    "        for phrase in ngrams:\n",
    "            doc = nlp_model(phrase)\n",
    "            results, found = extract_entities(doc, nlp_model, row_id, text)\n",
    "            if found:\n",
    "                all_results.extend(results)\n",
    "                found_any = True\n",
    "    return all_results, found_any\n",
    "\n",
    "# Read input CSV -PROVIDE THE INPUT FILE\n",
    "input_df = pd.read_csv(\"filtered_mimic_d_items.csv\", keep_default_na=False)\n",
    "\n",
    "entities = []\n",
    "\n",
    "for index, row in input_df.iterrows():\n",
    "    row_id = row['itemid']\n",
    "    text = row['label'].title()  # Normalize casing here\n",
    "    \n",
    "    found_cui = False\n",
    "    # Try both models in order, stop if found any entities\n",
    "    for nlp_model in [scilg_nlp, scibert_nlp]:\n",
    "        results, found = sliding_window_linking(text, nlp_model, row_id, max_n=3)\n",
    "        if found:\n",
    "            entities.extend(results)\n",
    "            found_cui = True\n",
    "            break\n",
    "\n",
    "# Convert to DataFrame\n",
    "entities_df = pd.DataFrame(entities)\n",
    "\n",
    "#dedup for overlapping n-gram\n",
    "entities_df = entities_df.drop_duplicates(subset=[\"Row_ID\", \"Text\", \"Start\", \"End\", \"UMLS_CUI\"])\n",
    "\n",
    "# Filter rows where LOINC_Part contains LP codes\n",
    "entities_df = entities_df[entities_df['LOINC_Part'] != \"No Mapping Found\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df57ecb",
   "metadata": {},
   "source": [
    "### pre processing of Loinc parts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54259954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the file that gives loincpart to unit maps for context ingetsion\n",
    "units_df = pd.read_csv(\"loinc_part_unit_prop.csv\", keep_default_na=False)\n",
    "\n",
    "#Explode LOINC_Part column\n",
    "entities_df = entities_df.assign(LOINC_Part=entities_df['LOINC_Part'].str.split(',')).explode('LOINC_Part')\n",
    "entities_df['LOINC_Part'] = entities_df['LOINC_Part'].str.strip()\n",
    "\n",
    "# Load the unwanted UMLS CUIs CSV and strip any leading/trailing spaces\n",
    "unwanted_umls_df = pd.read_csv('unwanted_system_cuis.csv')\n",
    "\n",
    "# Remove rows where UMLS_CUI exists in the unwanted CSV (unwanted_umls_df)\n",
    "# use ~ (negation) to keep only the rows where UMLS_CUI is NOT in the unwanted CSV\n",
    "entities_df = entities_df[~entities_df['UMLS_CUI'].isin(unwanted_umls_df['UMLS_CUI'])]\n",
    "\n",
    "# Step 4: Match units from units_df against Text (case-insensitive), add new rows\n",
    "units_df = units_df.copy()\n",
    "units_df['units_len'] = units_df['units'].str.len()\n",
    "units_df = units_df.sort_values('units_len', ascending=False)\n",
    "\n",
    "new_rows = []\n",
    "matched_row_ids = set()\n",
    "\n",
    "for _, unit_row in units_df.iterrows():\n",
    "    unit = str(unit_row['units']).strip().lower()\n",
    "    partnumber = unit_row['partnumber']\n",
    "\n",
    "    if not unit:\n",
    "        continue\n",
    "\n",
    "    # Handle '%' as a special case (literal substring match)\n",
    "    if unit == '%':\n",
    "        matches = entities_df[entities_df['Text'].str.contains('%', case=False, na=False)]\n",
    "    else:\n",
    "        pattern = re.escape(unit)\n",
    "        matches = entities_df[entities_df['Text'].str.lower().str.contains(pattern, na=False, regex=True)]\n",
    "\n",
    "    for idx, match_row in matches.iterrows():\n",
    "        # If this row is already matched by a longer unit, skip it\n",
    "        if idx in matched_row_ids:\n",
    "            continue\n",
    "        matched_row_ids.add(idx)\n",
    "\n",
    "        new_rows.append({\n",
    "            \"Row_ID\": match_row[\"Row_ID\"],\n",
    "            \"Text\": match_row[\"Text\"],\n",
    "            \"Start\": None,\n",
    "            \"End\": None,\n",
    "            \"Label\": None,\n",
    "            \"UMLS_CUI\": None,\n",
    "            \"UMLS_Name\": None,\n",
    "            \"Score\": None,\n",
    "            \"LOINC_Part\": partnumber\n",
    "        })\n",
    "# Append new rows (if any) to entities_df\n",
    "if new_rows:\n",
    "    additional_df = pd.DataFrame(new_rows)\n",
    "    entities_df = pd.concat([entities_df, additional_df], ignore_index=True)\n",
    "\n",
    "#remove redundant rows\n",
    "entities_df = entities_df.drop_duplicates(subset=[\"Row_ID\", \"Text\", \"UMLS_CUI\", \"LOINC_Part\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6e64e",
   "metadata": {},
   "source": [
    "### Concerting from LOINC part to LOINC code candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from rapidfuzz.fuzz import partial_ratio\n",
    "\n",
    "def print_size(df, msg):\n",
    "    print(f\"{msg} -> rows: {len(df)}, columns: {len(df.columns)}, memory (MB): {df.memory_usage(deep=True).sum() / (1024*1024):.2f}\")\n",
    "\n",
    "# ---------- Step 1: Read Data ----------\n",
    "loinc_part = pd.read_csv(\"LoincPartLink_Primary.csv\")\n",
    "loinc = pd.read_csv('Loinc.csv', dtype=str)\n",
    "loinc['COMMON_TEST_RANK'] = pd.to_numeric(loinc['COMMON_TEST_RANK'], errors='coerce')\n",
    "\n",
    "loinc = loinc[['LOINC_NUM', 'COMPONENT', 'PROPERTY', 'SYSTEM', 'METHOD_TYP', 'LONG_COMMON_NAME', 'COMMON_TEST_RANK']]\n",
    "loinc.rename(columns={'METHOD_TYP': 'METHOD'}, inplace=True)\n",
    "\n",
    "# Lowercase columns\n",
    "entities_df.columns = [col.lower() for col in entities_df.columns]\n",
    "loinc_part.columns = [col.lower() for col in loinc_part.columns]\n",
    "entities_df = entities_df.rename(columns={'loinc_part': 'partnumber'})\n",
    "\n",
    "# ---------- Step 2: Merge Entities with LOINC Parts ----------\n",
    "observation_parts = entities_df.merge(loinc_part, on='partnumber')\n",
    "\n",
    "# ---------- Step 3: Pivot ----------\n",
    "pivot = observation_parts.pivot_table(\n",
    "    index='row_id',\n",
    "    columns='parttypename',\n",
    "    values='partname',\n",
    "    aggfunc=lambda x: list(set(x))\n",
    ").reset_index()\n",
    "\n",
    "print_size(pivot, \"Pivot after merge\")\n",
    "\n",
    "pivot.columns = [col if isinstance(col, str) else col[1] for col in pivot.columns]\n",
    "print(pivot.columns.tolist())\n",
    "\n",
    "# ---------- Step 4: Explode COMPONENT, PROPERTY, SYSTEM, METHOD_TYP ----------\n",
    "cols_to_explode = ['COMPONENT', 'PROPERTY', 'SYSTEM', 'METHOD']\n",
    "for col in cols_to_explode:\n",
    "    if col in pivot.columns:\n",
    "        pivot[col] = pivot[col].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "        pivot = pivot.explode(col).reset_index(drop=True)\n",
    "\n",
    "print_size(pivot, \"Pivot after explode\")\n",
    "\n",
    "# ---------- Step 5: Prepare Permutations ----------\n",
    "# Define all possible axes that could be used\n",
    "all_possible_axes = ['COMPONENT', 'PROPERTY', 'SYSTEM', 'METHOD']\n",
    "\n",
    "# Dynamically detect join columns that exist in BOTH pivot and loinc\n",
    "join_columns = [col for col in all_possible_axes if col in pivot.columns and col in loinc.columns]\n",
    "print(\"Join columns actually used:\", join_columns)\n",
    "\n",
    "# Generate all permutations of those columns for matching priority\n",
    "all_perms = []\n",
    "for r in range(len(join_columns), 0, -1):\n",
    "    all_perms.extend(list(combinations(join_columns, r)))\n",
    "\n",
    "print(\"All permutations:\", all_perms)\n",
    "\n",
    "# ---------- Step 6: Optimize Data Types ----------\n",
    "for col in join_columns:\n",
    "    loinc[col] = pd.Categorical(loinc[col])\n",
    "    pivot[col] = pd.Categorical(pivot[col])\n",
    "\n",
    "# ---------- Step 7: Batch Processing ----------\n",
    "final_results = []\n",
    "\n",
    "unique_row_ids = pivot['row_id'].unique()\n",
    "chunk_size = 500  # tune based on memory\n",
    "\n",
    "for start in range(0, len(unique_row_ids), chunk_size):\n",
    "    batch_ids = unique_row_ids[start:start + chunk_size]\n",
    "    batch_df = pivot[pivot['row_id'].isin(batch_ids)]\n",
    "\n",
    "    batch_output = []\n",
    "\n",
    "    for rid in batch_df['row_id'].unique():\n",
    "        row_data = batch_df[batch_df['row_id'] == rid]\n",
    "\n",
    "        for perm in all_perms:\n",
    "            perm = list(perm)\n",
    "\n",
    "            if not set(perm).issubset(set(row_data.columns)):\n",
    "                continue\n",
    "\n",
    "            if row_data[perm].isnull().any(axis=None):\n",
    "                continue\n",
    "\n",
    "            dedup = row_data.drop_duplicates(subset=['row_id'] + perm)\n",
    "            joined = dedup.merge(loinc, on=perm)\n",
    "\n",
    "            if not joined.empty:\n",
    "                joined['join_strength'] = len(perm)\n",
    "                joined['join_on'] = ', '.join(perm)\n",
    "                batch_output.append(joined)\n",
    "                break  # stop after highest match\n",
    "\n",
    "    if batch_output:\n",
    "        batch_result = pd.concat(batch_output, ignore_index=True)\n",
    "        final_results.append(batch_result)\n",
    "\n",
    "    print(f\"Processed batch {start} to {start + chunk_size}\")\n",
    "\n",
    "# ---------- Step 8: Combine All Batches ----------\n",
    "if final_results:\n",
    "    final_df = pd.concat(final_results, ignore_index=True)\n",
    "else:\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "print_size(final_df, \"Final joined dataframe\")\n",
    "\n",
    "# ---------- NEW STEP 8.5: Compute Robust Composite Score ----------\n",
    "\n",
    "# Get max join_strength from final_df (handle case if empty)\n",
    "max_join_strength = final_df['join_strength'].max() if not final_df.empty else 1\n",
    "\n",
    "# Precompute row_id â†’ input text map\n",
    "row_id_to_text = (\n",
    "    entities_df.dropna(subset=['text'])\n",
    "    .groupby('row_id')['text']\n",
    "    .first()\n",
    "    .str.lower()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "def input_aware_axis_score(row):\n",
    "    # Normalize join_strength to [0,1]\n",
    "    axis_match_score = row['join_strength'] / max_join_strength if max_join_strength > 0 else 0\n",
    "\n",
    "    rank = row['COMMON_TEST_RANK']\n",
    "    if pd.isna(rank) or rank <= 0:\n",
    "        rank_score = 0\n",
    "    else:\n",
    "        max_rank = 30000\n",
    "        rank_score = 1 - (rank / max_rank)\n",
    "        rank_score = max(rank_score, 0)\n",
    "\n",
    "    # Fuzzy similarity score (input text vs LOINC name)\n",
    "    input_text = row_id_to_text.get(row['row_id'], '').lower()\n",
    "    loinc_name = str(row.get('LONG_COMMON_NAME', '')).lower()\n",
    "    if input_text and loinc_name:\n",
    "        sim_score = partial_ratio(input_text, loinc_name) / 100  # Normalize to 0â€“1\n",
    "    else:\n",
    "        sim_score = 0\n",
    "\n",
    "    # Weights\n",
    "    w_axis = 0.5\n",
    "    w_rank = 0.3\n",
    "    w_sim = 0.3\n",
    "\n",
    "    return w_axis * axis_match_score  + w_rank * rank_score + w_sim * sim_score\n",
    "\n",
    "\n",
    "# Apply composite score\n",
    "final_df['composite_score'] = final_df.apply(input_aware_axis_score, axis=1)\n",
    "\n",
    "# Sort by composite score\n",
    "#final_df = final_df.sort_values(by=['row_id', 'composite_score'], ascending=[True, False])\n",
    "\n",
    "print_size(final_df, \"Final dataframe after composite scoring\")\n",
    "#---end of new section\n",
    "\n",
    "\n",
    "# ---------- Step 9: Sorting + Top 10 ----------\n",
    "final_df['sort_rank'] = pd.to_numeric(final_df['COMMON_TEST_RANK'], errors='coerce').replace(0, np.nan)\n",
    "\n",
    "final_df = (\n",
    "    final_df.sort_values(by=['row_id', 'join_strength', 'sort_rank'], ascending=[True, False, True])\n",
    "    .groupby('row_id')\n",
    "    .head(10)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "final_df = final_df.drop(columns=['sort_rank'])\n",
    "\n",
    "print_size(final_df, \"Final dataframe after sorting\")\n",
    "\n",
    "# ----------  Done  ----------\n",
    "\n",
    "# Apply hardcoded LOINC mappings to existing rows OF HEMOGRAM RELATED STRINGS\n",
    "\n",
    "# Define hardcoded mappings\n",
    "hardcoded_mappings = {\n",
    "    'mch': {\n",
    "        'LOINC_NUM': '28539-5',\n",
    "        'LONG_COMMON_NAME': 'MCH [Entitic mass]',\n",
    "        'COMMON_TEST_RANK': 104,\n",
    "        'join_strength': 1,\n",
    "        'join_on': 'BROAD',\n",
    "        'composite_score': '0.9'\n",
    "    },\n",
    "    'mcv': {\n",
    "        'LOINC_NUM': '787-2',\n",
    "        'LONG_COMMON_NAME': 'MCV [Entitic mean volume] in Red Blood Cells by Automated count',\n",
    "        'COMMON_TEST_RANK': 14,\n",
    "        'join_strength': 1,\n",
    "        'join_on': 'BROAD',\n",
    "        'composite_score': '0.9'\n",
    "    },\n",
    "    'mchc': {\n",
    "        'LOINC_NUM': '786-4',\n",
    "        'LONG_COMMON_NAME': 'MCHC [Entitic Mass/volume] in Red Blood Cells by Automated count',\n",
    "        'COMMON_TEST_RANK': 13,\n",
    "        'join_strength': 1,\n",
    "        'join_on': 'BROAD',\n",
    "        'composite_score': '0.9'\n",
    "    },\n",
    "    'hct': {\n",
    "        'LOINC_NUM': '4544-3',\n",
    "        'LONG_COMMON_NAME': 'Hematocrit [Volume Fraction] of Blood by Automated count',\n",
    "        'COMMON_TEST_RANK': 28,\n",
    "        'join_strength': 1,\n",
    "        'join_on': 'BROAD',\n",
    "        'composite_score': '0.9'\n",
    "    },\n",
    "    'heart rate': {\n",
    "        'LOINC_NUM': '8867-4',\n",
    "        'LONG_COMMON_NAME': 'Heart rate',\n",
    "        'COMMON_TEST_RANK': 18,\n",
    "        'join_strength': 1,\n",
    "        'join_on': 'BROAD',\n",
    "        'composite_score': '0.9'\n",
    "    },\n",
    "    'fetal heart rate': {\n",
    "        'LOINC_NUM': '55283-6',\n",
    "        'LONG_COMMON_NAME': 'Fetal Heart rate',\n",
    "        'COMMON_TEST_RANK': 2568,\n",
    "        'join_strength': 1,\n",
    "        'join_on': 'BROAD',\n",
    "        'composite_score': '0.9'\n",
    "    },\n",
    "    'weight': {\n",
    "        'LOINC_NUM': '29463-7',\n",
    "        'LONG_COMMON_NAME': 'Body weight',\n",
    "        'COMMON_TEST_RANK': 44,\n",
    "        'join_strength': 1,\n",
    "        'join_on': 'BROAD',\n",
    "        'composite_score': '0.9'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Iterate through each keyword and update matching rows\n",
    "for keyword, values in hardcoded_mappings.items():\n",
    "    mask = entities_df['text'].str.lower().str.contains(keyword, na=False)\n",
    "    matching_row_ids = entities_df.loc[mask, 'row_id'].tolist()\n",
    "    \n",
    "    if matching_row_ids:\n",
    "        update_mask = final_df['row_id'].isin(matching_row_ids)\n",
    "        \n",
    "        final_df.loc[update_mask, 'LOINC_NUM'] = values['LOINC_NUM']\n",
    "        final_df.loc[update_mask, 'LONG_COMMON_NAME'] = values['LONG_COMMON_NAME']\n",
    "        final_df.loc[update_mask, 'COMMON_TEST_RANK'] = values['COMMON_TEST_RANK']\n",
    "        final_df.loc[update_mask, 'join_strength'] = values['join_strength']\n",
    "        final_df.loc[update_mask, 'join_on'] = values['join_on']\n",
    "        final_df.loc[update_mask, 'composite_score'] = values['composite_score']\n",
    "\n",
    "print_size(final_df, \"Final dataframe after hardcoded updates\")\n",
    "\n",
    "final_df = final_df.drop_duplicates(\n",
    "    subset=['row_id', 'LOINC_NUM', 'join_on']\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc3d07",
   "metadata": {},
   "source": [
    "### final get the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce98c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final view - Deduplicate to keep one text per row_id\n",
    "entity_texts = entities_df[['row_id', 'text']].drop_duplicates()\n",
    "\n",
    "#Merge with top 15 LOINC matches from final_df\n",
    "entities_top15 = entity_texts.merge(\n",
    "    final_df[['row_id', 'LOINC_NUM', 'LONG_COMMON_NAME', 'COMMON_TEST_RANK', 'join_strength', 'join_on', 'composite_score']],\n",
    "    how='left',\n",
    "    on='row_id'\n",
    ")\n",
    "\n",
    "#Reset index (optional for cleanliness)\n",
    "entities_top15 = entities_top15.reset_index(drop=True)\n",
    "\n",
    "#optional export if needed\n",
    "merged_df.to_csv(\"mapping file.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
